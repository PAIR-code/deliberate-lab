/**
 * Test assumes a container with ollama is running on port 11434
 * Download the docker image to run :
 * https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image
 */

const MODEL = "llama3.2"
const LLM_SERVER_ENDPOINT = "http://localhost:11434/api/chat"

